{
    "ollama": {
        "use_model": false,
        "model_name": "llama3.1:8b",
        "temperature": 1.0,
        "max_iterations": 20
    },
    "deepseek": {
        "use_model": false,
        "model_name": "deepseek-chat",
        "temperature": 1.0,
        "max_iterations": 20
    },
    "qwen3": {
        "use_model": true,
        "model_name": "qwen3-max",
        "temperature": 1.0,
        "max_iterations": 20
    }


}
